{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "百度网盘AI大赛-文档检测优化赛第4名方案\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义dataloader\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import paddle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "\n",
    "def train_augmentation():\n",
    "    train_transform = [\n",
    "        # A.RandomSizedCrop(min_max_height=(1024, 1024), height=1024, width=1024, w2h_ratio=1.0, always_apply=False, p=1.),\n",
    "        A.Flip(p=0.5),\n",
    "        A.Rotate(p=0.5),\n",
    "        ############################ add #################\n",
    "        # A.RandomGridShuffle(grid=(2, 2), p=0.2),\n",
    "        #\n",
    "        # A.Transpose(p=0.5),\n",
    "        # A.ShiftScaleRotate(p=0.5),\n",
    "        ############################ add #################\n",
    "\n",
    "        # A.OneOf([\n",
    "        #     A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.),\n",
    "        #     A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=1.),\n",
    "        #     A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.),\n",
    "        # ], p=0.5),\n",
    "\n",
    "    ]\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "class MyDateset(paddle.io.Dataset):\n",
    "    def __init__(self, mode='train', train_imgs_dir='D:/yyc/competition/AIstudio/Document_detection/train_datasets_document_detection_0411/images/', transform=train_augmentation(),\n",
    "                 label_imgs_dir='D:/yyc/competition/AIstudio/Document_detection/train_datasets_document_detection_0411/segments/', train_txt='D:/yyc/competition/AIstudio/Document_detection/train_datasets_document_detection_0411/data_info.txt'):\n",
    "        super(MyDateset, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        self.train_imgs_dir = train_imgs_dir\n",
    "        self.label_imgs_dir = label_imgs_dir\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(train_txt, 'r') as f:\n",
    "            self.train_infor = f.readlines()\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.train_infor[index][:-1]\n",
    "        splited = item.split(',')\n",
    "        img_name = splited[0]\n",
    "\n",
    "        img = cv2.imread(self.train_imgs_dir + img_name + '.jpg')\n",
    "        h, w, c = img.shape\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        lab = cv2.imread(self.label_imgs_dir + img_name + '.png')\n",
    "\n",
    "        trans = self.transform(image=img, mask=lab)\n",
    "        img, lab = trans['image'], trans['mask']\n",
    "\n",
    "\n",
    "        lab = paddle.vision.transforms.resize(lab, (768, 768), interpolation='nearest')\n",
    "        lab = (lab[:, :, 0] == 255)\n",
    "\n",
    "        # plt.imshow(lab)\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        # 对图片进行resize，调整明暗对比度等参数\n",
    "        img = paddle.vision.transforms.resize(img, (768, 768), interpolation='bilinear')\n",
    "        if np.random.rand() < 1 / 2:\n",
    "            img = paddle.vision.transforms.adjust_brightness(img, np.random.rand() * 2)\n",
    "        else:\n",
    "            if np.random.rand() < 1 / 2:\n",
    "                img = paddle.vision.transforms.adjust_contrast(img, np.random.rand() * 2)\n",
    "            else:\n",
    "                img = paddle.vision.transforms.adjust_hue(img, np.random.rand() - 0.5)\n",
    "\n",
    "\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = img / 255\n",
    "\n",
    "\n",
    "\n",
    "        img = paddle.to_tensor(img).astype('float32')\n",
    "        label = paddle.to_tensor(lab).astype('int64')\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_infor)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import paddle.nn.initializer as paddle_init\n",
    "\n",
    "\n",
    "# from paddleseg.cvlibs import manager\n",
    "# from paddleseg.utils import utils\n",
    "# from paddleseg.models.backbones.transformer_utils import *\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, pretrained_model):\n",
    "    if pretrained_model is not None:\n",
    "        print('Loading pretrained model from {}'.format(pretrained_model))\n",
    "\n",
    "        # if urlparse(pretrained_model).netloc:\n",
    "        #     pretrained_model = download_pretrained_model(pretrained_model)\n",
    "\n",
    "        if os.path.exists(pretrained_model):\n",
    "            para_state_dict = paddle.load(pretrained_model)\n",
    "\n",
    "            model_state_dict = model.state_dict()\n",
    "            keys = model_state_dict.keys()\n",
    "            num_params_loaded = 0\n",
    "            for k in keys:\n",
    "                if k not in para_state_dict:\n",
    "                    print(\"{} is not in pretrained model\".format(k))\n",
    "                elif list(para_state_dict[k].shape) != list(model_state_dict[k]\n",
    "                                                            .shape):\n",
    "                    print(\n",
    "                        \"[SKIP] Shape of pretrained params {} doesn't match.(Pretrained: {}, Actual: {})\"\n",
    "                        .format(k, para_state_dict[k].shape, model_state_dict[k]\n",
    "                                .shape))\n",
    "                else:\n",
    "                    model_state_dict[k] = para_state_dict[k]\n",
    "                    num_params_loaded += 1\n",
    "            model.set_dict(model_state_dict)\n",
    "            print(\"There are {}/{} variables loaded into {}.\".format(\n",
    "                num_params_loaded,\n",
    "                len(model_state_dict), model.__class__.__name__))\n",
    "\n",
    "        else:\n",
    "            raise ValueError('The pretrained model directory is not Found: {}'.\n",
    "                             format(pretrained_model))\n",
    "    else:\n",
    "        print(\n",
    "            'No pretrained model to load, {} will be trained from scratch.'.\n",
    "            format(model.__class__.__name__))\n",
    "\n",
    "\n",
    "\n",
    "def to_2tuple(x):\n",
    "    return tuple([x] * 2)\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob=0., training=False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\n",
    "    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n",
    "    random_tensor = paddle.floor(random_tensor)  # binarize\n",
    "    output = x.divide(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Layer):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class Identity(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "trunc_normal_ = paddle_init.TruncatedNormal(std=.02)\n",
    "zeros_ = paddle_init.Constant(value=0.)\n",
    "ones_ = paddle_init.Constant(value=1.)\n",
    "\n",
    "\n",
    "def init_weights(layer):\n",
    "    \"\"\"\n",
    "    Init the weights of transformer.\n",
    "    Args:\n",
    "        layer(nn.Layer): The layer to init weights.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        trunc_normal_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            zeros_(layer.bias)\n",
    "    elif isinstance(layer, nn.LayerNorm):\n",
    "        zeros_(layer.bias)\n",
    "        ones_(layer.weight)\n",
    "\n",
    "\n",
    "class Mlp(nn.Layer):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.reshape(\n",
    "        [B, H // window_size, window_size, W // window_size, window_size, C])\n",
    "    windows = x.transpose([0, 1, 3, 2, 4,\n",
    "                           5]).reshape([-1, window_size, window_size, C])\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.reshape(\n",
    "        [B, H // window_size, W // window_size, window_size, window_size, -1])\n",
    "    x = x.transpose([0, 1, 3, 2, 4, 5]).reshape([B, H, W, -1])\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Layer):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 window_size,\n",
    "                 num_heads,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = self.create_parameter(\n",
    "            shape=((2 * window_size[0] - 1) * (2 * window_size[1] - 1),\n",
    "                   num_heads),\n",
    "            default_initializer=zeros_)\n",
    "        self.add_parameter(\"relative_position_bias_table\",\n",
    "                           self.relative_position_bias_table)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = paddle.arange(self.window_size[0])\n",
    "        coords_w = paddle.arange(self.window_size[1])\n",
    "        coords = paddle.stack(paddle.meshgrid([coords_h,\n",
    "                                               coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = paddle.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        coords_flatten_1 = coords_flatten.unsqueeze(axis=2)\n",
    "        coords_flatten_2 = coords_flatten.unsqueeze(axis=1)\n",
    "        relative_coords = coords_flatten_1 - coords_flatten_2\n",
    "\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "\n",
    "        relative_coords[:, :, 0] += self.window_size[\n",
    "            0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(\n",
    "            [B_, N, 3, self.num_heads,\n",
    "             C // self.num_heads]).transpose([2, 0, 3, 1, 4])\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = paddle.mm(q, k.transpose([0, 1, 3, 2]))\n",
    "\n",
    "        index = self.relative_position_index.reshape([-1])\n",
    "        relative_position_bias = paddle.index_select(\n",
    "            self.relative_position_bias_table, index)\n",
    "\n",
    "        relative_position_bias = relative_position_bias.reshape([\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1], -1\n",
    "        ])  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.transpose(\n",
    "            [2, 0, 1])  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.reshape([B_ // nW, nW, self.num_heads, N, N\n",
    "                                 ]) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.reshape([-1, self.num_heads, N, N])\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = paddle.mm(attn, v).transpose([0, 2, 1, 3]).reshape([B_, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Layer):\n",
    "    \"\"\"\n",
    "    Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Layer, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 shift_size=0,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim,\n",
    "                       hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape([B, H, W, C])\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "\n",
    "        x = x.transpose([0, 3, 1, 2])\n",
    "        x = F.pad(x, [pad_l, pad_r, pad_t, pad_b])\n",
    "        x = x.transpose([0, 2, 3, 1])\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = paddle.roll(\n",
    "                x, shifts=(-self.shift_size, -self.shift_size), axis=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(\n",
    "            shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.reshape(\n",
    "            [-1, self.window_size * self.window_size,\n",
    "             C])  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(\n",
    "            x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.reshape(\n",
    "            [-1, self.window_size, self.window_size, C])\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp,\n",
    "                                   Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = paddle.roll(\n",
    "                shifted_x,\n",
    "                shifts=(self.shift_size, self.shift_size),\n",
    "                axis=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :]\n",
    "\n",
    "        x = x.reshape([B, H * W, C])\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Layer):\n",
    "    \"\"\"\n",
    "    Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias_attr=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.reshape([B, H, W, C])\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = x.transpose([0, 3, 1, 2])\n",
    "            x = F.pad(x, [0, W % 2, 0, H % 2])\n",
    "            x = x.transpose([0, 2, 3, 1])\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = paddle.concat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.reshape([B, -1, 4 * C])  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Layer):\n",
    "    \"\"\"\n",
    "    A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of feature channels.\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Layer | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.LayerList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i]\n",
    "                if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = paddle.zeros((1, Hp, Wp, 1))  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(\n",
    "            img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.reshape(\n",
    "            [-1, self.window_size * self.window_size])\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "\n",
    "        huns = -100.0 * paddle.ones_like(attn_mask)\n",
    "        attn_mask = huns * (attn_mask != 0).astype(\"float32\")\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Layer):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding.\n",
    "    Args:\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2D(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.shape\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, [0, self.patch_size[1] - W % self.patch_size[1], 0, 0])\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, [0, 0, 0, self.patch_size[0] - H % self.patch_size[0]])\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            _, _, Wh, Ww = x.shape\n",
    "            x = x.flatten(2).transpose([0, 2, 1])\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose([0, 2, 1]).reshape([-1, self.embed_dim, Wh, Ww])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Layer):\n",
    "    \"\"\"\n",
    "    The SwinTransformer implementation based on PaddlePaddle.\n",
    "    The original article refers to\n",
    "    Liu, Ze, et al. \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"\n",
    "    (https://arxiv.org/abs/2103.14030)\n",
    "    Args:\n",
    "        pretrain_img_size (int): Input image size for training the pretrained model, used in absolute postion embedding. Default: 224.\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "        num_heads (tuple[int]): Number of attention head of each stage.\n",
    "        window_size (int): Window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "        norm_layer (nn.Layer): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "        out_indices (Sequence[int]): Output from which stages.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode). -1 means not freezing any parameters. Default: -1.\n",
    "        pretrained (str, optional): The path or url of pretrained model. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrain_img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=96,\n",
    "                 depths=[2, 2, 6, 2],\n",
    "                 num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 out_indices=(0, 1, 2, 3),\n",
    "                 frozen_stages=-1,\n",
    "                 pretrained=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.out_indices = out_indices\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        # self.patch_embed = nn.Conv2D(3, 96, 7, 1, 3)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            patches_resolution = [\n",
    "                pretrain_img_size[0] // patch_size[0],\n",
    "                pretrain_img_size[1] // patch_size[1]\n",
    "            ]\n",
    "\n",
    "            self.absolute_pos_embed = self.create_parameter(\n",
    "                shape=(1, embed_dim, patches_resolution[0],\n",
    "                       patches_resolution[1]),\n",
    "                default_initializer=zeros_)\n",
    "            self.add_parameter(\"absolute_pos_embed\", self.absolute_pos_embed)\n",
    "            trunc_normal_(self.absolute_pos_embed)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = np.linspace(0, drop_path_rate, sum(depths)).tolist()\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.LayerList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging\n",
    "                if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        feat_channels = [int(embed_dim * 2**i) for i in range(self.num_layers)]\n",
    "        self.feat_channels = feat_channels\n",
    "\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in out_indices:\n",
    "            layer = norm_layer(feat_channels[i_layer])\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_sublayer(layer_name, layer)\n",
    "\n",
    "        self._freeze_stages()\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.init_weights(self.pretrained)\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1 and self.ape:\n",
    "            self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 2:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages - 1):\n",
    "                layer = self.layers[i]\n",
    "                layer.eval()\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "        if pretrained is not None:\n",
    "            load_pretrained_model(self, self.pretrained)\n",
    "        else:\n",
    "            for sublayer in self.sublayers():\n",
    "                if isinstance(sublayer, nn.Linear):\n",
    "                    trunc_normal_(sublayer.weight)\n",
    "                    if isinstance(sublayer,\n",
    "                                  nn.Linear) and sublayer.bias is not None:\n",
    "                        zeros_(sublayer.bias)\n",
    "                elif isinstance(sublayer, nn.LayerNorm):\n",
    "                    zeros_(sublayer.bias)\n",
    "                    ones_(sublayer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        _, _, Wh, Ww = x.shape\n",
    "        if self.ape:\n",
    "            # interpolate the position embedding to the corresponding size\n",
    "            absolute_pos_embed = F.interpolate(\n",
    "                self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "        else:\n",
    "            x = x.flatten(2).transpose([0, 2, 1])\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "                norm_layer = getattr(self, f'norm{i}')\n",
    "                x_out = norm_layer(x_out)\n",
    "\n",
    "                out = x_out.reshape(\n",
    "                    [-1, H, W, self.feat_channels[i]]).transpose([0, 3, 1, 2])\n",
    "                outs.append(out)\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    # def train(self):\n",
    "    #     \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "    #     super(SwinTransformer, self).train()\n",
    "    #     self._freeze_stages()\n",
    "\n",
    "class FPNBlock(nn.Layer):\n",
    "    def __init__(self, pyramid_channels, skip_channels):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2D(skip_channels, pyramid_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        skip = self.skip_conv(skip)\n",
    "        x = x + skip\n",
    "        return x\n",
    "\n",
    "class Conv3x3GNReLU(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, upsample=False):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2D(\n",
    "                in_channels, out_channels, (3, 3), stride=1, padding=1, bias_attr=False\n",
    "            ),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        return x\n",
    "\n",
    "class SegmentationBlock(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, n_upsamples=0):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples))]\n",
    "\n",
    "        if n_upsamples > 1:\n",
    "            for _ in range(1, n_upsamples):\n",
    "                blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n",
    "\n",
    "        self.block = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MergeBlock(nn.Layer):\n",
    "    def __init__(self, policy):\n",
    "        super().__init__()\n",
    "        if policy not in [\"add\", \"cat\"]:\n",
    "            raise ValueError(\n",
    "                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(\n",
    "                    policy\n",
    "                )\n",
    "            )\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.policy == 'add':\n",
    "            return sum(x)\n",
    "        elif self.policy == 'cat':\n",
    "            return paddle.concat(x, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(self.policy)\n",
    "            )\n",
    "\n",
    "\n",
    "class FPNDecoder(nn.Layer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            encoder_depth=5,\n",
    "            pyramid_channels=256,\n",
    "            segmentation_channels=128,\n",
    "            dropout=0.1,\n",
    "            merge_policy=\"add\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = segmentation_channels if merge_policy == \"add\" else segmentation_channels * 4\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(encoder_depth))\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[:encoder_depth + 1]\n",
    "\n",
    "        self.p5 = nn.Conv2D(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n",
    "        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n",
    "        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n",
    "\n",
    "        self.seg_blocks = nn.LayerList([\n",
    "            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n",
    "            for n_upsamples in [3, 2, 1, 0]\n",
    "        ])\n",
    "\n",
    "        self.merge = MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2D(p=dropout)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        c2, c3, c4, c5 = features[-4:]\n",
    "\n",
    "        p5 = self.p5(c5)\n",
    "        p4 = self.p4(p5, c4)\n",
    "        p3 = self.p3(p4, c3)\n",
    "        p2 = self.p2(p3, c2)\n",
    "\n",
    "        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p5, p4, p3, p2])]\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class swimb_fpn(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 encoder='resnet34',\n",
    "                 encoder_depth=4,\n",
    "                 in_channels=3,\n",
    "                 pretrained=True,\n",
    "                 classes=2,\n",
    "                 decoder_pyramid_channels: int = 256,\n",
    "                 decoder_segmentation_channels: int = 128,\n",
    "                 decoder_merge_policy: str = \"add\",\n",
    "                 decoder_dropout: float = 0.2,\n",
    "                 pretrain=None,\n",
    "                 ):\n",
    "        super(swimb_fpn, self).__init__()\n",
    "\n",
    "        # ENCODER\n",
    "        self.encoder = SwinTransformer(\n",
    "        pretrain_img_size=384,\n",
    "        embed_dim=128,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[4, 8, 16, 32],\n",
    "        window_size=12,\n",
    "        pretrained=pretrain\n",
    "        )\n",
    "        encoder_channels = [128, 256, 512, 1024]\n",
    "\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder = FPNDecoder(\n",
    "            encoder_channels=encoder_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "        )\n",
    "\n",
    "        # PREDICT\n",
    "        self.pred_head = nn.Conv2D(decoder_segmentation_channels, classes, 3, 1, 1)\n",
    "        # self.up = nn.Conv2DTranspose(in_channels=256,\n",
    "        #                              out_channels=128,\n",
    "        #                              kernel_size=2,\n",
    "        #                              stride=2,\n",
    "        #                              padding=0)\n",
    "        # self.conv1 = nn.Conv2D(in_channels=128,\n",
    "        #                        out_channels=64,\n",
    "        #                        kernel_size=3,\n",
    "        #                        stride=1,\n",
    "        #                        padding=1)\n",
    "        # self.bn = nn.BatchNorm(64, act=\"relu\")\n",
    "        # self.up2 = nn.Conv2DTranspose(in_channels=64,\n",
    "        #                              out_channels=classes,\n",
    "        #                              kernel_size=2,\n",
    "        #                              stride=2,\n",
    "        #                              padding=0)\n",
    "        # self.conv2 = nn.Conv2D(in_channels=classes,\n",
    "        #                        out_channels=classes,\n",
    "        #                        kernel_size=31,\n",
    "        #                        stride=1,\n",
    "        #                        padding=15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_feats = self.encoder(x)\n",
    "\n",
    "        decoder_feat = self.decoder(*encoder_feats)\n",
    "        out = self.pred_head(decoder_feat)\n",
    "        out = F.interpolate(out, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        # out = self.up(out)\n",
    "        # out = self.bn(self.conv1(out))\n",
    "        # out = self.up2(out)\n",
    "        # out = self.conv2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义loss函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# from paddleseg.cvlibs import manager\n",
    "\n",
    "\n",
    "# @manager.LOSSES.add_component\n",
    "class DiceLoss(nn.Layer):\n",
    "    \"\"\"\n",
    "    The implements of the dice loss.\n",
    "\n",
    "    Args:\n",
    "        weight (list[float], optional): The weight for each class. Default: None.\n",
    "        ignore_index (int64): ignore_index (int64, optional): Specifies a target value that\n",
    "            is ignored and does not contribute to the input gradient. Default ``255``.\n",
    "        smooth (float32): Laplace smoothing to smooth dice loss and accelerate convergence.\n",
    "            Default: 1.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None, ignore_index=255, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smooth = smooth\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        num_class = logits.shape[1]\n",
    "        if self.weight is not None:\n",
    "            assert num_class == len(self.weight), \\\n",
    "                \"The lenght of weight should be euqal to the num class\"\n",
    "\n",
    "        logits = F.softmax(logits, axis=1)\n",
    "        labels_one_hot = F.one_hot(labels, num_class)\n",
    "        labels_one_hot = paddle.transpose(labels_one_hot, [0, 3, 1, 2])\n",
    "\n",
    "        mask = labels != self.ignore_index\n",
    "        mask = paddle.cast(paddle.unsqueeze(mask, 1), 'float32')\n",
    "\n",
    "        dice_loss = 0.0\n",
    "        for i in range(num_class):\n",
    "            dice_loss_i = dice_loss_helper(logits[:, i], labels_one_hot[:, i],\n",
    "                                           mask, self.smooth, self.eps)\n",
    "            if self.weight is not None:\n",
    "                dice_loss_i *= self.weight[i]\n",
    "            dice_loss += dice_loss_i\n",
    "        dice_loss = dice_loss / num_class\n",
    "\n",
    "        return dice_loss\n",
    "\n",
    "\n",
    "def dice_loss_helper(logit, label, mask, smooth, eps):\n",
    "    assert logit.shape == label.shape, \\\n",
    "        \"The shape of logit and label should be the same\"\n",
    "    logit = paddle.reshape(logit, [0, -1])\n",
    "    label = paddle.reshape(label, [0, -1])\n",
    "    mask = paddle.reshape(mask, [0, -1])\n",
    "    logit *= mask\n",
    "    label *= mask\n",
    "    intersection = paddle.sum(logit * label, axis=1)\n",
    "    cardinality = paddle.sum(logit + label, axis=1)\n",
    "    dice_loss = 1 - (2 * intersection + smooth) / (cardinality + smooth + eps)\n",
    "    dice_loss = dice_loss.mean()\n",
    "    return dice_loss\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# from paddleseg.cvlibs import manager\n",
    "\n",
    "\n",
    "# @manager.LOSSES.add_component\n",
    "class LovaszSoftmaxLoss(nn.Layer):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss.\n",
    "\n",
    "    Args:\n",
    "        ignore_index (int64): Specifies a target value that is ignored and does not contribute to the input gradient. Default ``255``.\n",
    "        classes (str|list): 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ignore_index=255, classes='present'):\n",
    "        super(LovaszSoftmaxLoss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.classes = classes\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        r\"\"\"\n",
    "        Forward computation.\n",
    "\n",
    "        Args:\n",
    "            logits (Tensor): Shape is [N, C, H, W], logits at each prediction (between -\\infty and +\\infty).\n",
    "            labels (Tensor): Shape is [N, 1, H, W] or [N, H, W], ground truth labels (between 0 and C - 1).\n",
    "        \"\"\"\n",
    "        probas = F.softmax(logits, axis=1)\n",
    "        vprobas, vlabels = flatten_probas(probas, labels, self.ignore_index)\n",
    "        loss = lovasz_softmax_flat(vprobas, vlabels, classes=self.classes)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# @manager.LOSSES.add_component\n",
    "class LovaszHingeLoss(nn.Layer):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss.\n",
    "\n",
    "    Args:\n",
    "        ignore_index (int64): Specifies a target value that is ignored and does not contribute to the input gradient. Default ``255``.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ignore_index=255):\n",
    "        super(LovaszHingeLoss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        r\"\"\"\n",
    "        Forward computation.\n",
    "\n",
    "        Args:\n",
    "            logits (Tensor): Shape is [N, 1, H, W] or [N, 2, H, W], logits at each pixel (between -\\infty and +\\infty).\n",
    "            labels (Tensor): Shape is [N, 1, H, W] or [N, H, W], binary ground truth masks (0 or 1).\n",
    "        \"\"\"\n",
    "        if logits.shape[1] == 2:\n",
    "            logits = binary_channel_to_unary(logits)\n",
    "        loss = lovasz_hinge_flat(\n",
    "            *flatten_binary_scores(logits, labels, self.ignore_index))\n",
    "        return loss\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors.\n",
    "    See Alg. 1 in paper.\n",
    "    \"\"\"\n",
    "    gts = paddle.sum(gt_sorted)\n",
    "    p = len(gt_sorted)\n",
    "\n",
    "    intersection = gts - paddle.cumsum(gt_sorted, axis=0)\n",
    "    union = gts + paddle.cumsum(1 - gt_sorted, axis=0)\n",
    "    jaccard = 1.0 - intersection.cast('float32') / union.cast('float32')\n",
    "\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def binary_channel_to_unary(logits, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Converts binary channel logits to unary channel logits for lovasz hinge loss.\n",
    "    \"\"\"\n",
    "    probas = F.softmax(logits, axis=1)\n",
    "    probas = probas[:, 1, :, :]\n",
    "    logits = paddle.log(probas + eps / (1 - probas + eps))\n",
    "    logits = logits.unsqueeze(1)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    r\"\"\"\n",
    "    Binary Lovasz hinge loss.\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): Shape is [P], logits at each prediction (between -\\infty and +\\infty).\n",
    "        labels (Tensor): Shape is [P], binary ground truth labels (0 or 1).\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels - 1.\n",
    "    signs.stop_gradient = True\n",
    "    errors = 1. - logits * signs\n",
    "    errors_sorted, perm = paddle.fluid.core.ops.argsort(errors, 'axis', 0,\n",
    "                                                        'descending', True)\n",
    "    errors_sorted.stop_gradient = False\n",
    "    gt_sorted = paddle.gather(labels, perm)\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    grad.stop_gradient = True\n",
    "    loss = paddle.sum(F.relu(errors_sorted) * grad)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case).\n",
    "    Remove labels according to 'ignore'.\n",
    "    \"\"\"\n",
    "    scores = paddle.reshape(scores, [-1])\n",
    "    labels = paddle.reshape(labels, [-1])\n",
    "    labels.stop_gradient = True\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = labels != ignore\n",
    "    valid_mask = paddle.reshape(valid, (-1, 1))\n",
    "    indexs = paddle.nonzero(valid_mask)\n",
    "    indexs.stop_gradient = True\n",
    "    vscores = paddle.gather(scores, indexs[:, 0])\n",
    "    vlabels = paddle.gather(labels, indexs[:, 0])\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss.\n",
    "\n",
    "    Args:\n",
    "        probas (Tensor): Shape is [P, C], class probabilities at each prediction (between 0 and 1).\n",
    "        labels (Tensor): Shape is [P], ground truth labels (between 0 and C - 1).\n",
    "        classes (str|list): 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.shape[1]\n",
    "    losses = []\n",
    "    classes_to_sum = list(range(C)) if classes in ['all', 'present'\n",
    "                                                   ] else classes\n",
    "    for c in classes_to_sum:\n",
    "        fg = paddle.cast(labels == c, probas.dtype)  # foreground for class c\n",
    "        if classes == 'present' and fg.sum() == 0:\n",
    "            continue\n",
    "        fg.stop_gradient = True\n",
    "        if C == 1:\n",
    "            if len(classes_to_sum) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = paddle.abs(fg - class_pred)\n",
    "        errors_sorted, perm = paddle.fluid.core.ops.argsort(errors, 'axis', 0,\n",
    "                                                            'descending', True)\n",
    "        errors_sorted.stop_gradient = False\n",
    "\n",
    "        fg_sorted = paddle.gather(fg, perm)\n",
    "        fg_sorted.stop_gradient = True\n",
    "\n",
    "        grad = lovasz_grad(fg_sorted)\n",
    "        grad.stop_gradient = True\n",
    "        loss = paddle.sum(errors_sorted * grad)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if len(classes_to_sum) == 1:\n",
    "        return losses[0]\n",
    "\n",
    "    losses_tensor = paddle.stack(losses)\n",
    "    mean_loss = paddle.mean(losses_tensor)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch.\n",
    "    \"\"\"\n",
    "    if len(probas.shape) == 3:\n",
    "        probas = paddle.unsqueeze(probas, axis=1)\n",
    "    C = probas.shape[1]\n",
    "    probas = paddle.transpose(probas, [0, 2, 3, 1])\n",
    "    probas = paddle.reshape(probas, [-1, C])\n",
    "    labels = paddle.reshape(labels, [-1])\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = labels != ignore\n",
    "    valid_mask = paddle.reshape(valid, [-1, 1])\n",
    "    indexs = paddle.nonzero(valid_mask)\n",
    "    indexs.stop_gradient = True\n",
    "    vprobas = paddle.gather(probas, indexs[:, 0])\n",
    "    vlabels = paddle.gather(labels, indexs[:, 0])\n",
    "    return vprobas, vlabels\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对网络进行训练"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import paddle\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "paddle.set_device('gpu')\n",
    "batch_s = 4\n",
    "output_model_dir = './swimb'\n",
    "if not os.path.exists(output_model_dir):\n",
    "    os.mkdir(output_model_dir)\n",
    "\n",
    "# log file\n",
    "file_path = './swimb/train_log.txt'\n",
    "f_log = open(file_path, 'a')\n",
    "\n",
    "######## model\n",
    "# model = get_unetrw()\n",
    "# model = get_swiml()\n",
    "model = swimb_fpn(pretrain=r'D:\\yyc\\yycpython\\Document_seg\\model\\SwinTransformer_base_patch4_window12_384_22kto1k_pretrained.pdparams')\n",
    "model.train()\n",
    "\n",
    "train_dataset=MyDateset()\n",
    "train_dataloader = paddle.io.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_s,\n",
    "    shuffle=True,\n",
    "    drop_last=False)\n",
    "\n",
    "\n",
    "max_epoch=260\n",
    "scheduler = paddle.optimizer.lr.CosineAnnealingDecay(learning_rate=0.001, T_max=max_epoch)\n",
    "opt = paddle.optimizer.Adam(learning_rate=scheduler, parameters=model.parameters())\n",
    "\n",
    "loss_lvz = LovaszSoftmaxLoss()\n",
    "loss_dice = DiceLoss()\n",
    "\n",
    "start_epoch = 0\n",
    "epoch_steps = train_dataset.__len__()//batch_s\n",
    "print('epoch_steps', epoch_steps)\n",
    "now_step = start_epoch*epoch_steps\n",
    "\n",
    "for epoch in range(start_epoch, max_epoch):\n",
    "    f_log.write('epoch:'+str(epoch)+'\\n')\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "        now_step+=1\n",
    "\n",
    "        img, label = data\n",
    "\n",
    "        pre = model(img)\n",
    "\n",
    "        # loss = softmax_with_cross_entropy(paddle.transpose(pre, [0, 2, 3, 1]), label_sig.unsqueeze(3))\n",
    "        loss1 = loss_lvz(pre, label)\n",
    "        loss2 = loss_dice(pre, label)\n",
    "\n",
    "        loss = 0.65 * loss1 + loss2 * 0.35\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_gradients()\n",
    "\n",
    "        scheduler.step(epoch + step/epoch_steps)\n",
    "\n",
    "        lr_update = opt.get_lr()\n",
    "        if now_step%10==0:\n",
    "            content_log = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time())) + ' ' + \"epoch: {}, batch: {}, loss is: {}, lossohem is: {},lr:{}\".format(epoch, step, ((loss1 + loss2) * 0.5).mean().numpy(), loss.mean().numpy(),lr_update)\n",
    "            print(content_log)\n",
    "            f_log.write(content_log + '\\n')\n",
    "    paddle.save(model.state_dict(), os.path.join(output_model_dir, 'model_' + str(epoch)+'.pdparams'))\n",
    "f_log.close()\n",
    "paddle.save(model.state_dict(), 'model.pdparams')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "对测试集进行推理"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "def process(src_image_dir, save_dir):\n",
    "\n",
    "    model = swimb_fpn()\n",
    "    model_state_dict = paddle.load('./model/model_137.pdparams')\n",
    "    model.set_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # json_results = []\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(src_image_dir, \"*.jpg\"))\n",
    "    for image_path in image_paths:\n",
    "        # do something\n",
    "        img = cv2.imread(image_path)\n",
    "        h, w, c = img.shape\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = paddle.vision.transforms.resize(img, (512, 512), interpolation='bilinear')\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = img / 255\n",
    "        img = paddle.to_tensor(img).astype('float32')\n",
    "        img = img.unsqueeze(0)\n",
    "        with paddle.no_grad():\n",
    "            s_predict_seg = model(img)\n",
    "\n",
    "            s_image_h = paddle.flip(img, axis=[3])\n",
    "            s_predict_h = model(s_image_h)\n",
    "            s_predict_h_seg = paddle.flip(s_predict_h, axis=[3])\n",
    "\n",
    "            s_image_v = paddle.flip(img, axis=[2])\n",
    "            s_predict_v = model(s_image_v)\n",
    "            s_predict_v_seg = paddle.flip(s_predict_v, axis=[2])\n",
    "\n",
    "            pre = 0.5 * s_predict_seg + 0.25 * s_predict_h_seg + 0.25 * s_predict_v_seg\n",
    "\n",
    "        # 保存结果图片\n",
    "        out_image = paddle.argmax(pre, axis=1).astype(float)\n",
    "        # print(out_image[0,200:300,200:300])\n",
    "        out_image[out_image == 1] = 255\n",
    "        out_image = paddle.fluid.layers.expand(out_image, expand_times=[3, 1, 1])\n",
    "        # print(out_image[0,200:300,200:300])\n",
    "        out_image = out_image.transpose((1, 2, 0)).numpy().astype(np.uint8)\n",
    "\n",
    "        out_image = paddle.vision.transforms.resize(out_image, (h, w), interpolation='bilinear')\n",
    "\n",
    "\n",
    "        save_path = os.path.join(save_dir, os.path.basename(image_path).replace(\".jpg\", \".png\"))\n",
    "        cv2.imwrite(save_path, out_image)\n",
    "\n",
    "src_image_dir = r'D:\\yyc\\competition\\AIstudio\\Document_detection\\document_detection_testB_dataset'\n",
    "save_dir = r'D:\\yyc\\competition\\AIstudio\\Document_detection\\results'\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "process(src_image_dir, save_dir)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}